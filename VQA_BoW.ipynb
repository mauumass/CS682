{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = 'data/'\n",
    "img_feat = data + 'VQA_image_features.h5'\n",
    "img_ids = data + 'image_ids_vqa.json'\n",
    "map_features_to_id = data + 'VQA_img_features2id.json'\n",
    "img_info = data + 'imgid2imginfo.json'\n",
    "\n",
    "questions_train = data + 'vqa_questions_train.json'\n",
    "questions_validation = data + 'vqa_questions_valid.json'\n",
    "questions_test = data + 'vqa_questions_test.json'\n",
    "\n",
    "annotations_train = data + 'vqa_annotations_train.json'\n",
    "annotations_validation = data + 'vqa_annotations_valid.json'\n",
    "annotations_test = data + 'vqa_annotations_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_images():\n",
    "    # load computed VQA image features from hdf5 file\n",
    "    image_features = np.asarray(h5py.File(img_feat, 'r')['img_features'])\n",
    "\n",
    "    # load IDs file\n",
    "    with open(img_ids, 'r') as file:\n",
    "        image_ids = json.load(file)['image_ids']\n",
    "\n",
    "    # load feature mapping file\n",
    "    with open(map_features_to_id, 'r') as file:\n",
    "        feature_mapping = json.load(file)['VQA_imgid2id']\n",
    "\n",
    "    # load info file\n",
    "    with open(img_info, 'r') as file:\n",
    "        image_info = json.load(file)\n",
    "\n",
    "    return image_ids, image_features, feature_mapping, image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text():\n",
    "    with open(questions_train, 'r') as file:\n",
    "        q_train = [[x['question'], x['image_id']] for x in json.load(file)['questions']]\n",
    "    with open(questions_validation, 'r') as file:\n",
    "        q_validation = [[x['question'], x['image_id']] for x in json.load(file)['questions']]\n",
    "    with open(questions_test, 'r') as file:\n",
    "        q_test = [[x['question'], x['image_id']] for x in json.load(file)['questions']]\n",
    "        \n",
    "    with open(annotations_train, 'r') as file:\n",
    "        a_train = [x['multiple_choice_answer'] for x in json.load(file)['annotations']]\n",
    "    with open(annotations_validation, 'r') as file:\n",
    "        a_validation = [x['multiple_choice_answer'] for x in json.load(file)['annotations']]\n",
    "    with open(annotations_test, 'r') as file:\n",
    "        a_test = [x['multiple_choice_answer']  for x in json.load(file)['annotations']]\n",
    "    \n",
    "    return q_train, q_validation, q_test, a_train, a_validation, a_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Display image from URL\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "\n",
    "def show_image(imgid2info, id):\n",
    "    img_name = 'temp-image.jpg'\n",
    "    request.urlretrieve(imgid2info[str(id)]['flickr_url'], img_name)\n",
    "\n",
    "    img = Image.open(img_name)\n",
    "    img.show()\n",
    "\n",
    "    os.remove(img_name)\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NUM_EPOCHS = 5\n",
    "#LEARNING_RATE = 0.001\n",
    "#RNDM_SEED = 42\n",
    "#torch.manual_seed(RNDM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_id_to_features(image_id):\n",
    "    feat = feature_mapping[str(image_id)]\n",
    "    return image_features[feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train, q_validation, q_test, a_train, a_validation, a_test = read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids, image_features, feature_mapping, image_info = read_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visual_feat_mapping['376397']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#img_id = 376397\n",
    "#h5_id = visual_feat_mapping[str(img_id)]\n",
    "#img_feat = img_features[h5_id]\n",
    "#print(img_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TO DO \n",
    "# Increase size of datasets\n",
    "\n",
    "train_len = int(0.1 * len(q_train)) \n",
    "validation_len = int(0.1 * len(q_validation))\n",
    "test_len =  int(0.1 * len(q_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_data():\n",
    "    # determine train data\n",
    "    train_text = []\n",
    "    train_images = []\n",
    "    for i in range(train_len): \n",
    "        train_text.append((q_train[i][0].split(), a_train[i]))\n",
    "        train_images.append(image_id_to_features(q_train[i][1]))\n",
    "\n",
    "    # determine validation data\n",
    "    validation_text = []\n",
    "    validation_images = []\n",
    "    for i in range(validation_len): \n",
    "        #if str(i) in visual_feat_mapping.keys():\n",
    "        validation_text.append((q_validation[i][0].split(), a_validation[i]))\n",
    "        validation_images.append(image_id_to_features(q_validation[i][1]))\n",
    "\n",
    "    # determine test data\n",
    "    test_text = []\n",
    "    test_images = []\n",
    "    for i in range(test_len): \n",
    "        #if str(i) in visual_feat_mapping.keys():\n",
    "        test_text.append((q_test[i][0].split(), a_test[i]))\n",
    "        test_images.append(image_id_to_features(q_test[i][1]))\n",
    "        \n",
    "    return train_text, train_images, validation_text, validation_images, test_text, test_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data(text_features, visual_features):\n",
    "    combined = [(text, visual) for text, visual in zip(text_features, visual_features)]\n",
    "    random.shuffle(combined)\n",
    "    return [text for (text, _) in combined], [visual for (_, visual) in combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabulary():\n",
    "    question_vocab = {}\n",
    "    annotation_vocab = {}\n",
    "    annotation_vocab_lookup = []\n",
    "    for question, answer in train_text + validation_text + test_text:\n",
    "        for word in question:\n",
    "            if word not in question_vocab:\n",
    "                question_vocab[word] = len(question_vocab)\n",
    "        if answer not in annotation_vocab:\n",
    "            annotation_vocab[answer] = len(annotation_vocab)\n",
    "            annotation_vocab_lookup.append(answer)\n",
    "    return question_vocab, annotation_vocab, annotation_vocab_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text, train_images, validation_text, validation_images, test_text, test_images = organize_data()\n",
    "question_vocab, annotation_vocab, annotation_vocab_lookup = vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3653\n",
      "1350\n",
      "5701\n"
     ]
    }
   ],
   "source": [
    "question_vocab_size = len(question_vocab) \n",
    "annotation_vocab_size = len(annotation_vocab)\n",
    "image_feature_length = len(train_images[0])\n",
    "input_vector_size = question_vocab_size + image_feature_length\n",
    "print(question_vocab_size)\n",
    "print(annotation_vocab_size)\n",
    "print(input_vector_size)\n",
    "#question_vocab_size+image_feature_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_image_vector(question, question_vocab, visual_features): \n",
    "    vec = torch.zeros(len(question_vocab)+len(visual_features))\n",
    "    #print(vec.dtype)\n",
    "    for word in question:\n",
    "        vec[question_vocab[word]] += 1\n",
    "    for i in range(len(visual_features)):\n",
    "        vec[i+len(question_vocab)] += visual_features[i]\n",
    "    return vec.view(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_target(label, annotation_vocab):\n",
    "    return torch.LongTensor([annotation_vocab[label]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, batch_size):\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "        ep_loss = 0\n",
    "        \n",
    "        for start in range(0, len(train_text), batch_size):\n",
    "            text_batch = train_text[start:start+batch_size]\n",
    "            image_batch = train_images[start:start+batch_size]\n",
    "            in_mat = torch.zeros(batch_size, input_vector_size)\n",
    "            out_vec = torch.zeros(batch_size, dtype=torch.long)\n",
    "        #counter=1 \n",
    "            if start % 1024 == 0:\n",
    "                print(start, \"/\", len(train_text))\n",
    "                \n",
    "            for i, ((instance, label), image_features) in enumerate(zip(*shuffle_data(text_batch, image_batch))):\n",
    "\n",
    "                visual_features = torch.from_numpy(image_features)\n",
    "                bow_vec = bow_image_vector(instance, question_vocab, visual_features)\n",
    "                target = make_target(label, annotation_vocab)\n",
    "                \n",
    "                in_mat[i] = bow_vec\n",
    "                out_vec[i] = target\n",
    "                \n",
    "            #label_vec[i] = label\n",
    "            #label_vec = torch.tensor([label]) # sort of works\n",
    "            #print(label_vec.shape)\n",
    "            \n",
    "            log_probs = bow_model(in_mat) \n",
    "\n",
    "            batch_loss = loss_function(log_probs, out_vec)\n",
    "            #print(loss)\n",
    "            ep_loss += batch_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(ep, ep_loss)\n",
    "        \n",
    "    return bow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_model = BoWClassifier(annotation_vocab_size, input_vector_size)\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(bow_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "0 tensor(845.5402, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "1 tensor(427.7195, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "2 tensor(246.8250, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "3 tensor(182.2072, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "4 tensor(147.5235, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "5 tensor(126.1784, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "6 tensor(110.9383, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "7 tensor(99.9860, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "8 tensor(91.0591, grad_fn=<ThAddBackward>)\n",
      "0 / 4806\n",
      "1024 / 4806\n",
      "2048 / 4806\n",
      "3072 / 4806\n",
      "4096 / 4806\n",
      "9 tensor(84.1635, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "#trained_model, all_losses = train_model()\n",
    "trained_model = train(num_epochs, batch_size)    \n",
    "#print(\"Trained BoW model:\\n\", trained_model)\n",
    "#print(\"Average loss of each epoch:\\n\", all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, text, image_features):\n",
    "    counter = 0\n",
    "    for (question, actual_answer), visual_features in zip(text, image_features):\n",
    "        visual_features = torch.from_numpy(visual_features)\n",
    "        bow_vec = bow_image_vector(question, question_vocab, visual_features)\n",
    "        log_probs = model(bow_vec)\n",
    "        value, index = torch.max(log_probs, 1)\n",
    "        index = index.data[0]\n",
    "\n",
    "        predicted_answer = annotation_vocab_lookup[index]\n",
    "        #print(\"Question\", question)\n",
    "        #print(\"Actual Answer\", actual_answer)\n",
    "        #print(\"Predicted Answer\", predicted_answer)\n",
    "         \n",
    "        if predicted_answer == actual_answer:\n",
    "            counter += 1\n",
    "            \n",
    "    accuracy = (float(counter) / len(text)) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.3491468997087\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(bow_model, train_text, train_images)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_len_q = 0\n",
    "#for i in range(len(train_text)):\n",
    "#    sum_len_q += len(train_text[i][0])\n",
    "#\n",
    "#sum_len_q /= len(train_text)\n",
    "#print(sum_len_q)\n",
    "#\n",
    "#count = 0\n",
    "#for i in range(len(train_text)):\n",
    "#    if len(train_text[i][0]) > 10:\n",
    "#        count += 1\n",
    "#print(count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TRAINING WITHOUT BATCHES. NOT USED\n",
    "\n",
    "def train_model():\n",
    "    current_loss = 0\n",
    "    losses = []\n",
    "    \n",
    "    for iter in range(1, num_epochs+1):\n",
    "        print(\"Epoch\", iter, \"/\", num_epochs)\n",
    "        counter = 1\n",
    "        for (instance, label), image_features in zip(*shuffle_data(train_text, train_images)):\n",
    "            if counter % 1000 == 0:\n",
    "                print(counter, \"/\", len(train_text))\n",
    "            counter += 1\n",
    "            bow_model.zero_grad()\n",
    "            #for i in range(len(image_features)):\n",
    "            #    print(image_features[i])\n",
    "            #print(instance)\n",
    "            #print(label)\n",
    "            visual_features = torch.from_numpy(image_features)\n",
    "            bow_vec = bow_image_vector(instance, question_vocab, visual_features)\n",
    "            print(bow_vec.shape)\n",
    "            target = make_target(label, annotation_vocab)\n",
    "            print(target)\n",
    "            \n",
    "            log_probs = bow_model(bow_vec)        \n",
    "            loss = loss_function(log_probs, target)\n",
    "            current_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        losses.append(current_loss / len(train_text))\n",
    "                \n",
    "        print(\"The average loss of epoch \", iter, \" is: \", str(current_loss / len(train_text)))\n",
    "        current_loss = 0\n",
    "        \n",
    "    return bow_model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question ['What', 'English', 'meal', 'is', 'this', 'likely', 'for?']\n",
    "Actual Answer tea\n",
    "Predicted Answer no\n",
    "Question ['What', 'color', 'is', 'his', 'uniform?']\n",
    "Actual Answer blue\n",
    "Predicted Answer blue\n",
    "Question ['Which', 'girl', 'is', 'wearing', 'glasses?']\n",
    "Actual Answer right\n",
    "Predicted Answer no\n",
    "Question ['What', 'is', 'the', 'person', 'doing?']\n",
    "Actual Answer sunbathing\n",
    "Predicted Answer yes\n",
    "Question ['How', 'does', 'the', 'weather', 'appear', 'in', 'this', 'photo?']\n",
    "Actual Answer sunny\n",
    "Predicted Answer no\n",
    "Question ['What', 'kind', 'of', 'facility', 'are', 'the', 'people', 'standing', 'in?']\n",
    "Actual Answer greenhouse\n",
    "Predicted Answer no\n",
    "Question ['What', 'shape', 'is', 'this?']\n",
    "Actual Answer octagon\n",
    "Predicted Answer red\n",
    "Question ['What', 'color', 'is', 'the', 'Frisbee', 'in', 'the', \"man's\", 'hand?']\n",
    "Actual Answer red\n",
    "Predicted Answer white\n",
    "Question ['What', 'is', 'this', 'person', 'riding?']\n",
    "Actual Answer motorcycle\n",
    "Predicted Answer no\n",
    "Question ['What', 'color', 'are', 'the', 'frames', 'of', 'the', 'glasses?']\n",
    "Actual Answer brown\n",
    "Predicted Answer brown\n",
    "Question ['What', 'is', 'the', 'dog', 'looking', 'out', 'of?']\n",
    "Actual Answer window\n",
    "Predicted Answer no\n",
    "Question ['How', 'many', 'people', 'in', 'the', 'shot?']\n",
    "Actual Answer 12\n",
    "Predicted Answer 2\n",
    "Question ['What', 'is', 'this', 'animal?']\n",
    "Actual Answer giraffe\n",
    "Predicted Answer giraffe\n",
    "Question ['What', 'is', 'lined', 'up', 'on', 'the', 'counter', 'behind', 'the', 'man?']\n",
    "Actual Answer wine bottles\n",
    "Predicted Answer no\n",
    "Question ['What', 'type', 'of', 'food', 'is', 'the', 'man', 'eating?']\n",
    "Actual Answer pizza\n",
    "Predicted Answer pizza\n",
    "Question ['Is', 'there', 'more', 'meat', 'or', 'vegetables', 'on', 'the', 'plate?']\n",
    "Actual Answer vegetables\n",
    "Predicted Answer no\n",
    "Question ['Where', 'is', 'the', 'man?']\n",
    "Actual Answer beach\n",
    "Predicted Answer no\n",
    "Question ['Is', 'this', 'a', 'board', 'game?']\n",
    "Actual Answer yes\n",
    "Predicted Answer no\n",
    "Question ['Is', 'the', 'photo', 'in', 'black', 'in', 'white?']\n",
    "Actual Answer yes\n",
    "Predicted Answer no\n",
    "Question ['Is', 'this', 'area', 'rural?']\n",
    "Actual Answer no\n",
    "Predicted Answer no\n",
    "Question ['Are', 'there', 'number', 'on', 'the', 'large', 'cubes?']\n",
    "Actual Answer yes\n",
    "Predicted Answer no\n",
    "Question ['Is', 'the', 'bus', 'parked?']\n",
    "Actual Answer no\n",
    "Predicted Answer no\n",
    "Question ['Of', 'what', 'airline', 'is', 'the', 'closest', 'plane', 'in', 'the', 'background?']\n",
    "Actual Answer world\n",
    "Predicted Answer no\n",
    "Question ['What', 'season', 'was', 'this', 'photo', 'likely', 'taken', 'in?']\n",
    "Actual Answer winter\n",
    "Predicted Answer no\n",
    "Question ['Can', 'you', 'see', 'the', 'desktop', 'of', 'the', 'computer?']\n",
    "Actual Answer yes\n",
    "Predicted Answer yes\n",
    "Question ['Is', 'there', 'a', 'stop', 'sign?']\n",
    "Actual Answer yes\n",
    "Predicted Answer no\n",
    "Question ['Is', 'the', 'plane', 'taking', 'off?']\n",
    "Actual Answer yes\n",
    "Predicted Answer yes\n",
    "Question ['What', 'kind', 'of', 'creature', 'is', 'on', 'the', 'right?']\n",
    "Actual Answer cat\n",
    "Predicted Answer no\n",
    "Question ['Are', 'the', 'giraffes', 'in', 'the', 'wild?']\n",
    "Actual Answer no\n",
    "Predicted Answer no\n",
    "Question ['What', 'is', 'the', 'boy', 'reaching', 'for?']\n",
    "Actual Answer banana\n",
    "Predicted Answer yes\n",
    "Question ['What', 'is', 'the', 'material', 'on', 'the', 'ground?']\n",
    "Actual Answer brick\n",
    "Predicted Answer yes\n",
    "Question ['Is', 'this', 'a', 'bat', 'or', 'golf', 'club?']\n",
    "Actual Answer neither\n",
    "Predicted Answer no\n",
    "Question ['Is', 'the', 'room', 'busy?']\n",
    "Actual Answer no\n",
    "Predicted Answer no\n",
    "Question ['Is', 'there', 'carrots', 'on', 'the', 'plate?']\n",
    "Actual Answer yes\n",
    "Predicted Answer yes\n",
    "Question ['Is', 'there', 'anything', 'in', 'this', 'picture', 'than', 'can', 'transfer', 'data', 'to', 'another', 'computer?']\n",
    "Actual Answer yes\n",
    "Predicted Answer no\n",
    "Question ['Is', 'there', 'a', 'sandy', 'beach', 'in', 'the', 'horizon?']\n",
    "Actual Answer no\n",
    "Predicted Answer no\n",
    "Question ['Are', 'these', 'types', 'of', 'planes', 'currently', 'used?']\n",
    "Actual Answer no\n",
    "Predicted Answer yes\n",
    "Question ['What', 'is', 'on', 'the', 'plate?']\n",
    "Actual Answer donuts\n",
    "Predicted Answer white\n",
    "Question ['How', 'many', 'tablecloths', 'are', 'there?']\n",
    "Actual Answer 2\n",
    "Predicted Answer 2\n",
    "Question ['Are', 'the', 'slices', 'of', 'pizza', 'small?']\n",
    "Actual Answer yes\n",
    "Predicted Answer yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
